{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from pathlib import Path\n",
    "from dn3.transforms.instance import To1020\n",
    "from dn3.configuratron import ExperimentConfig\n",
    "import mne\n",
    "import parse\n",
    "from dn3.utils import DN3ConfigException\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoaderERPBCI:\n",
    "    \"\"\"\n",
    "    The dataset from https://physionet.org/content/erpbci/1.0.0/ required a customized solution.\n",
    "\n",
    "    I've put it in an object so that the solution is somewhat self-contained.\n",
    "    \"\"\"\n",
    "    MAX_ACCEPTABLE_FLASHES = 144\n",
    "    SOA = 0.15\n",
    "    TOTAL_RUN_TIME_S = int(MAX_ACCEPTABLE_FLASHES * SOA)\n",
    "    STIM_CHANNEL = 'STI 014'\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_target_and_crop(raw):\n",
    "        target_char = parse.search('#Tgt{}_', raw.annotations[0]['description'])[0]\n",
    "\n",
    "        # Find the first speller flash (it isn't consistently at the second or even nth index for that matter)\n",
    "        start_off = 0\n",
    "        while len(raw.annotations[start_off]['description']) > 6 and start_off < len(raw.annotations):\n",
    "            start_off += 1\n",
    "        assert start_off < len(raw.annotations) - 1\n",
    "        start_t = raw.annotations[start_off]['onset']\n",
    "        end_t = start_t + LoaderERPBCI.TOTAL_RUN_TIME_S\n",
    "        # Operates in-place\n",
    "        raw.crop(start_t, end_t, include_tmax=False)\n",
    "        return target_char\n",
    "\n",
    "    @staticmethod\n",
    "    def _make_blank_stim(raw):\n",
    "        info = mne.create_info([LoaderERPBCI.STIM_CHANNEL], raw.info['sfreq'], ['stim'])\n",
    "        stim_raw = mne.io.RawArray(np.zeros((1, len(raw.times))), info)\n",
    "        raw.add_channels([stim_raw], force_update_info=True)\n",
    "\n",
    "    @classmethod\n",
    "    def __call__(cls, path: Path):\n",
    "        # Data has to be preloaded to add events to it, swap edf for fif if haven't offline processed first\n",
    "        # run = mne.io.read_raw_edf(str(path), preload=True)\n",
    "        run = mne.io.read_raw_fif(str(path), preload=True)\n",
    "        if len(run.annotations) == 0:\n",
    "            raise DN3ConfigException\n",
    "        cls._make_blank_stim(run)\n",
    "        target_letter = cls._get_target_and_crop(run)\n",
    "        events, occurrences = mne.events_from_annotations(run, lambda a: int(target_letter in a) + 1)\n",
    "        run.add_events(events, stim_channel=cls.STIM_CHANNEL)\n",
    "        return run\n",
    "\n",
    "CUSTOM_LOADERS = dict(\n",
    "    erpbci=LoaderERPBCI,\n",
    ")\n",
    "\n",
    "def get_ds(name, ds):\n",
    "    if name in CUSTOM_LOADERS:\n",
    "        ds.add_custom_raw_loader(CUSTOM_LOADERS[name]())\n",
    "    dataset = ds.auto_construct_dataset()\n",
    "    dataset.add_transform(To1020())\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_lmoso_iterator(name, ds):\n",
    "    dataset = get_ds(name, ds)\n",
    "    specific_test = ds.test_subjects if hasattr(ds, 'test_subjects') else None\n",
    "    return dataset.loso(test_person_id=specific_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding additional configuration entries: dict_keys(['extensions', 'train_params', 'lr'])\n",
      "Configuratron found 1 datasets.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning /scratch/s194260/bci_iv. If there are a lot of files, this may take a while...: 100%|██████████| 1/1 [00:00<00:00, 628.64it/s, extension=.edf]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset of 9 Preloaded Epoched recordings from 1 people.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Datasets:   0%|          | 0/1 [00:00<?, ?it/s]                         \n",
      "Datasets:   0%|          | 0/1 [00:00<?, ?it/s]                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping A02T.edf. Exception: ('No stim channels found, but the raw object has annotations. Consider using mne.events_from_annotations to convert these to events.',).\n",
      "Skipping A06T.edf. Exception: ('No stim channels found, but the raw object has annotations. Consider using mne.events_from_annotations to convert these to events.',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Datasets:   0%|          | 0/1 [00:00<?, ?it/s]                         \n",
      "Datasets:   0%|          | 0/1 [00:00<?, ?it/s]                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping A03T.edf. Exception: ('No stim channels found, but the raw object has annotations. Consider using mne.events_from_annotations to convert these to events.',).\n",
      "Skipping A07T.edf. Exception: ('No stim channels found, but the raw object has annotations. Consider using mne.events_from_annotations to convert these to events.',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Datasets:   0%|          | 0/1 [00:00<?, ?it/s]                         \n",
      "Datasets:   0%|          | 0/1 [00:00<?, ?it/s]                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping A09T.edf. Exception: ('No stim channels found, but the raw object has annotations. Consider using mne.events_from_annotations to convert these to events.',).\n",
      "Skipping A01T.edf. Exception: ('No stim channels found, but the raw object has annotations. Consider using mne.events_from_annotations to convert these to events.',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Datasets:   0%|          | 0/1 [00:01<?, ?it/s]                         \n",
      "Datasets:   0%|          | 0/1 [00:01<?, ?it/s]                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping A05T.edf. Exception: ('No stim channels found, but the raw object has annotations. Consider using mne.events_from_annotations to convert these to events.',).\n",
      "Skipping A04T.edf. Exception: ('No stim channels found, but the raw object has annotations. Consider using mne.events_from_annotations to convert these to events.',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Datasets:   0%|          | 0/1 [00:01<?, ?it/s]                         \n",
      "Loading BCI Competition IV 2a: 100%|██████████| 1/1 [00:01<00:00,  1.32s/person]\n",
      "Datasets:   0%|          | 0/1 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping A08T.edf. Exception: ('No stim channels found, but the raw object has annotations. Consider using mne.events_from_annotations to convert these to events.',).\n",
      "None of the sessions for bci_iv were usable. Skipping...\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "datasets should not be an empty iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/agjma/BENDR_ORIGINAL/dataloader.ipynb Cell 3\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btitans.compute.dtu.dk/home/agjma/BENDR_ORIGINAL/dataloader.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m experiment \u001b[39m=\u001b[39m ExperimentConfig(\u001b[39m'\u001b[39m\u001b[39mdownstream_tasks.yml\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btitans.compute.dtu.dk/home/agjma/BENDR_ORIGINAL/dataloader.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m ds_name, ds \u001b[39min\u001b[39;00m tqdm\u001b[39m.\u001b[39mtqdm(experiment\u001b[39m.\u001b[39mdatasets\u001b[39m.\u001b[39mitems(), total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(experiment\u001b[39m.\u001b[39mdatasets\u001b[39m.\u001b[39mitems()), desc\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mDatasets\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Btitans.compute.dtu.dk/home/agjma/BENDR_ORIGINAL/dataloader.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mfor\u001b[39;00m training, validation, test \u001b[39min\u001b[39;00m get_lmoso_iterator(ds_name, ds):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btitans.compute.dtu.dk/home/agjma/BENDR_ORIGINAL/dataloader.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btitans.compute.dtu.dk/home/agjma/BENDR_ORIGINAL/dataloader.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     thinkers \u001b[39m=\u001b[39m training\u001b[39m.\u001b[39mdatasets[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;32m/home/agjma/BENDR_ORIGINAL/dataloader.ipynb Cell 3\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btitans.compute.dtu.dk/home/agjma/BENDR_ORIGINAL/dataloader.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=57'>58</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_lmoso_iterator\u001b[39m(name, ds):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Btitans.compute.dtu.dk/home/agjma/BENDR_ORIGINAL/dataloader.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=58'>59</a>\u001b[0m     dataset \u001b[39m=\u001b[39m get_ds(name, ds)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btitans.compute.dtu.dk/home/agjma/BENDR_ORIGINAL/dataloader.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=59'>60</a>\u001b[0m     specific_test \u001b[39m=\u001b[39m ds\u001b[39m.\u001b[39mtest_subjects \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(ds, \u001b[39m'\u001b[39m\u001b[39mtest_subjects\u001b[39m\u001b[39m'\u001b[39m) \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btitans.compute.dtu.dk/home/agjma/BENDR_ORIGINAL/dataloader.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=60'>61</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m dataset\u001b[39m.\u001b[39mloso(test_person_id\u001b[39m=\u001b[39mspecific_test)\n",
      "\u001b[1;32m/home/agjma/BENDR_ORIGINAL/dataloader.ipynb Cell 3\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btitans.compute.dtu.dk/home/agjma/BENDR_ORIGINAL/dataloader.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m CUSTOM_LOADERS:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btitans.compute.dtu.dk/home/agjma/BENDR_ORIGINAL/dataloader.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m     ds\u001b[39m.\u001b[39madd_custom_raw_loader(CUSTOM_LOADERS[name]())\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Btitans.compute.dtu.dk/home/agjma/BENDR_ORIGINAL/dataloader.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m dataset \u001b[39m=\u001b[39m ds\u001b[39m.\u001b[39;49mauto_construct_dataset()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btitans.compute.dtu.dk/home/agjma/BENDR_ORIGINAL/dataloader.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m dataset\u001b[39m.\u001b[39madd_transform(To1020())\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btitans.compute.dtu.dk/home/agjma/BENDR_ORIGINAL/dataloader.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m \u001b[39mreturn\u001b[39;00m dataset\n",
      "File \u001b[0;32m~/.conda/envs/BENDR/lib/python3.8/site-packages/dn3/configuratron/config.py:627\u001b[0m, in \u001b[0;36mDatasetConfig.auto_construct_dataset\u001b[0;34m(self, mapping, **dsargs)\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    603\u001b[0m \u001b[39mThis creates a dataset using the config values. If tlen and tmin are specified in the config, creates epoched\u001b[39;00m\n\u001b[1;32m    604\u001b[0m \u001b[39mdataset, otherwise Raw.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[39m        An instance of :any:`Dataset`, constructed according to mapping.\u001b[39;00m\n\u001b[1;32m    625\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    626\u001b[0m \u001b[39mif\u001b[39;00m mapping \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 627\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mauto_construct_dataset(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mauto_mapping(), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdsargs)\n\u001b[1;32m    629\u001b[0m file_types \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mRaw\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_raw_recordings \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mEpoched\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreload:\n",
      "File \u001b[0;32m~/.conda/envs/BENDR/lib/python3.8/site-packages/dn3/configuratron/config.py:649\u001b[0m, in \u001b[0;36mDatasetConfig.auto_construct_dataset\u001b[0;34m(self, mapping, **dsargs)\u001b[0m\n\u001b[1;32m    647\u001b[0m dsargs\u001b[39m.\u001b[39msetdefault(\u001b[39m'\u001b[39m\u001b[39mdataset_id\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset_id)\n\u001b[1;32m    648\u001b[0m dsargs\u001b[39m.\u001b[39msetdefault(\u001b[39m'\u001b[39m\u001b[39mreturn_trial_id\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_trial_ids)\n\u001b[0;32m--> 649\u001b[0m dataset \u001b[39m=\u001b[39m Dataset(thinkers, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdsargs)\n\u001b[1;32m    650\u001b[0m \u001b[39mprint\u001b[39m(dataset)\n\u001b[1;32m    651\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeep1010 \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/BENDR/lib/python3.8/site-packages/dn3/data/dataset.py:747\u001b[0m, in \u001b[0;36mDataset.__init__\u001b[0;34m(self, thinkers, dataset_id, task_id, return_trial_id, return_session_id, return_person_id, return_dataset_id, return_task_id, dataset_info)\u001b[0m\n\u001b[1;32m    745\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39msorted\u001b[39m(thinkers\u001b[39m.\u001b[39mkeys()):\n\u001b[1;32m    746\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__add__\u001b[39m(thinkers[t], person_id\u001b[39m=\u001b[39mt, return_session_id\u001b[39m=\u001b[39mreturn_session_id, return_trial_id\u001b[39m=\u001b[39mreturn_trial_id)\n\u001b[0;32m--> 747\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reset_dataset()\n\u001b[1;32m    749\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset_id \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(dataset_id)\u001b[39m.\u001b[39mlong() \u001b[39mif\u001b[39;00m dataset_id \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    750\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtask_id \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(task_id)\u001b[39m.\u001b[39mlong() \u001b[39mif\u001b[39;00m task_id \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/BENDR/lib/python3.8/site-packages/dn3/data/dataset.py:786\u001b[0m, in \u001b[0;36mDataset._reset_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    784\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mthinkers[p_id]\u001b[39m.\u001b[39msessions[s_id]\u001b[39m.\u001b[39msession_id \u001b[39m=\u001b[39m s_id\n\u001b[1;32m    785\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mthinkers[p_id]\u001b[39m.\u001b[39msessions[s_id]\u001b[39m.\u001b[39mperson_id \u001b[39m=\u001b[39m p_id\n\u001b[0;32m--> 786\u001b[0m ConcatDataset\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mthinkers\u001b[39m.\u001b[39;49mvalues())\n",
      "File \u001b[0;32m~/.conda/envs/BENDR/lib/python3.8/site-packages/torch/utils/data/dataset.py:199\u001b[0m, in \u001b[0;36mConcatDataset.__init__\u001b[0;34m(self, datasets)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39msuper\u001b[39m(ConcatDataset, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m    198\u001b[0m \u001b[39m# Cannot verify that datasets is Sized\u001b[39;00m\n\u001b[0;32m--> 199\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(datasets) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdatasets should not be an empty iterable\u001b[39m\u001b[39m'\u001b[39m  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdatasets \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(datasets)\n\u001b[1;32m    201\u001b[0m \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdatasets:\n",
      "\u001b[0;31mAssertionError\u001b[0m: datasets should not be an empty iterable"
     ]
    }
   ],
   "source": [
    "experiment = ExperimentConfig('downstream_tasks.yml')\n",
    "\n",
    "for ds_name, ds in tqdm.tqdm(experiment.datasets.items(), total=len(experiment.datasets.items()), desc='Datasets'):\n",
    "    for training, validation, test in get_lmoso_iterator(ds_name, ds):\n",
    "        break\n",
    "    \n",
    "    thinkers = training.datasets[0]\n",
    "    epochs = thinkers.datasets[0]\n",
    "    X, y = epochs.to_numpy()\n",
    "    print(\"Labels: \", y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BENDR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
