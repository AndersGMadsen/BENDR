{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3,4\"\n",
    "\n",
    "import objgraph\n",
    "\n",
    "import time\n",
    "import utils\n",
    "from result_tracking import ThinkerwiseResultTracker\n",
    "\n",
    "import dn3\n",
    "from dn3.configuratron import ExperimentConfig\n",
    "from dn3.data.dataset import Thinker\n",
    "from dn3.trainable.processes import StandardClassification\n",
    "\n",
    "from dn3_ext import BENDRClassification, LinearHeadBENDR\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding additional configuration entries: dict_keys(['train_params', 'lr', 'folds'])\n",
      "Configuratron found 1 datasets.\n"
     ]
    }
   ],
   "source": [
    "experiment = ExperimentConfig(\"configs/downstream.yml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = experiment.datasets[\"mmidb\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_name, ds = list(experiment.datasets.items())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning data/datasets/eegmmidb. If there are a lot of files, this may take a while...: 100%|██████████| 4/4 [00:00<00:00, 12.89it/s, extension=.gdf]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset of 315 Preloaded Epoched recordings from 105 people.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Physionet MMIDB: 100%|██████████| 105/105 [00:13<00:00,  7.53person/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Physionet MMIDB | DSID: None | 105 people | 4408 trials | 90 channels | 1536 samples/trial | 256Hz | 0 transforms\n",
      "Constructed 1 channel maps\n",
      "====================\n",
      "Used by 315 recordings:\n",
      "EEG (original(new)): Fc5.(FC5) Fc3.(FC3) Fc1.(FC1) Fcz.(FCZ) Fc2.(FC2) Fc4.(FC4) Fc6.(FC6) C5..(C5) C3..(C3) C1..(C1) Cz..(CZ) C2..(C2) C4..(C4) C6..(C6) Cp5.(CP5) Cp3.(CP3) Cp1.(CP1) Cpz.(CPZ) Cp2.(CP2) Cp4.(CP4) Cp6.(CP6) Fp1.(FP1) Fpz.(FPZ) Fp2.(FP2) Af7.(AF7) Af3.(AF3) Afz.(AFZ) Af4.(AF4) Af8.(AF8) F7..(F7) F5..(F5) F3..(F3) F1..(F1) Fz..(FZ) F2..(F2) F4..(F4) F6..(F6) F8..(F8) Ft7.(FT7) Ft8.(FT8) T7..(T7) T8..(T8) T9..(T9) T10.(T10) Tp7.(TP7) Tp8.(TP8) P7..(P7) P5..(P5) P3..(P3) P1..(P1) Pz..(PZ) P2..(P2) P4..(P4) P6..(P6) P8..(P8) Po7.(PO7) Po3.(PO3) Poz.(POZ) Po4.(PO4) Po8.(PO8) O1..(O1) Oz..(OZ) O2..(O2) Iz..(IZ) \n",
      "EOG (original(new)): \n",
      "REF (original(new)): \n",
      "EXTRA (original(new)): \n",
      "Heuristically Assigned: Fc5.(FC5)  Fc3.(FC3)  Fc1.(FC1)  Fcz.(FCZ)  Fc2.(FC2)  Fc4.(FC4)  Fc6.(FC6)  C5..(C5)  C3..(C3)  C1..(C1)  Cz..(CZ)  C2..(C2)  C4..(C4)  C6..(C6)  Cp5.(CP5)  Cp3.(CP3)  Cp1.(CP1)  Cpz.(CPZ)  Cp2.(CP2)  Cp4.(CP4)  Cp6.(CP6)  Fp1.(FP1)  Fpz.(FPZ)  Fp2.(FP2)  Af7.(AF7)  Af3.(AF3)  Afz.(AFZ)  Af4.(AF4)  Af8.(AF8)  F7..(F7)  F5..(F5)  F3..(F3)  F1..(F1)  Fz..(FZ)  F2..(F2)  F4..(F4)  F6..(F6)  F8..(F8)  Ft7.(FT7)  Ft8.(FT8)  T7..(T7)  T8..(T8)  T9..(T9)  T10.(T10)  Tp7.(TP7)  Tp8.(TP8)  P7..(P7)  P5..(P5)  P3..(P3)  P1..(P1)  Pz..(PZ)  P2..(P2)  P4..(P4)  P6..(P6)  P8..(P8)  Po7.(PO7)  Po3.(PO3)  Poz.(POZ)  Po4.(PO4)  Po8.(PO8)  O1..(O1)  Oz..(OZ)  O2..(O2)  Iz..(IZ) \n",
      "--------------------\n",
      "Excluded []\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "gen = utils.get_lmoso_iterator(ds_name, ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:   >> Physionet MMIDB | DSID: None | 63 people | 2646 trials | 20 channels | 1536 samples/trial | 256Hz | 1 transforms\n",
      "Validation: >> Physionet MMIDB | DSID: None | 21 people | 880 trials | 20 channels | 1536 samples/trial | 256Hz | 1 transforms\n",
      "Test:       >> Physionet MMIDB | DSID: None | 21 people | 882 trials | 20 channels | 1536 samples/trial | 256Hz | 1 transforms\n"
     ]
    }
   ],
   "source": [
    "training, validation, test = next(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Batches: 100%|██████████| 42/42 [00:05<00:00,  7.72it/s]\n"
     ]
    }
   ],
   "source": [
    "x, y = training.to_numpy()\n",
    "x = torch.Tensor(x)\n",
    "y = torch.Tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BENDR(BENDRClassification):\n",
    "    def __init__(self, targets, samples, channels, encoder_h=512, contextualizer_hidden=3076, projection_head=False,\n",
    "                 new_projection_layers=0, dropout=0., trial_embeddings=None, layer_drop=0, keep_layers=None,\n",
    "                 mask_p_t=0.01, mask_p_c=0.005, mask_t_span=0.1, mask_c_span=0.1, multi_gpu=False):\n",
    "        super().__init__(targets, samples, channels, encoder_h, contextualizer_hidden, projection_head,\n",
    "                 new_projection_layers, dropout, trial_embeddings, layer_drop, keep_layers,\n",
    "                 mask_p_t, mask_p_c, mask_t_span, mask_c_span, multi_gpu)\n",
    "        \n",
    "        self.input_shape = torch.Size((channels, samples))\n",
    "        self.output_shape = torch.Size((targets, ))\n",
    "        \n",
    "    def load_classifier(self, classifier_file, freeze=False):\n",
    "        classifier_state_dict = torch.load(classifier_file)\n",
    "        self.classifier.load_state_dict(classifier_state_dict, strict=True)\n",
    "\n",
    "        for param in self.classifier.parameters():\n",
    "            param.requires_grad = not freeze\n",
    "\n",
    "    def load_all(self, encoder_file: str, contextualizer_file: str, classifier_file: str, strict=True,\n",
    "                 freeze_encoder=False, freeze_contextualizer=False, freeze_classifier=False):\n",
    "        encoder_state_dict = torch.load(encoder_file)\n",
    "        self.encoder.load_state_dict(encoder_state_dict, strict=strict)\n",
    "        self.encoder.freeze_features(unfreeze=not freeze_encoder)\n",
    "\n",
    "        contextualizer_state_dict = torch.load(contextualizer_file)\n",
    "        self.contextualizer.load_state_dict(contextualizer_state_dict, strict=True)\n",
    "        self.contextualizer.freeze_features(unfreeze=not freeze_contextualizer)\n",
    "\n",
    "        self.load_classifier(classifier_file, freeze=freeze_classifier)\n",
    "\n",
    "    def save_all(self, encoder_file: str, contextualizer_file: str, classifier_file: str):\n",
    "        torch.save(self.encoder.state_dict(), encoder_file)\n",
    "        torch.save(self.contextualizer.state_dict(), contextualizer_file)\n",
    "        torch.save(self.classifier.state_dict(), classifier_file)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, return_features: bool = False, grad: bool = False):\n",
    "        assert isinstance(x, torch.Tensor), \"Input has to be of instance torch.Tensor\"\n",
    "\n",
    "        if x.shape == self.input_shape: x = torch.unsqueeze(x, dim=0)\n",
    "        \n",
    "        assert x.shape[1:] == self.input_shape, \"Input has to be of shape (*, {}, {})\".format(*self.input_shape)\n",
    "\n",
    "        self.return_features = return_features\n",
    "        \n",
    "        if grad:\n",
    "            output = super().forward(x)\n",
    "        else:\n",
    "            with torch.no_grad(): output = super().forward(x)\n",
    "\n",
    "        if return_features:\n",
    "            return output[0], output[1]\n",
    "        else:\n",
    "            return output\n",
    "        \n",
    "    def forward_probs(self, x: torch.Tensor, return_features: bool = False, grad: bool = False):\n",
    "        output = self.forward(x, return_features, grad)\n",
    "\n",
    "        if return_features:\n",
    "            return output[0].softmax(dim=1), output[1]\n",
    "        else:\n",
    "            return output.softmax(dim=1)\n",
    "\n",
    "    def evaluate(self, X, batch_size = 8, return_probs = False):\n",
    "        assert isinstance(X, torch.Tensor), \"Input has to be of instance torch.Tensor\"\n",
    "        assert X.shape[1:] == self.input_shape, \"Input has to be of shape (*, {}, {})\".format(*self.input_shape)\n",
    "\n",
    "        N = len(X)\n",
    "\n",
    "        probs = torch.empty((N, self.targets))\n",
    "\n",
    "        prog_bar = tqdm(total = int(torch.ceil(torch.Tensor([len(X) / batch_size])).item()),\n",
    "                        desc=\"Evaluating\", unit=\"batches\")\n",
    "\n",
    "        for i, x in zip(range(0, N, batch_size), X.split(batch_size)):\n",
    "            probs[i:(i + batch_size)] = self.forward_probs(x)\n",
    "            prog_bar.update(1)\n",
    "\n",
    "        prog_bar.close()\n",
    "\n",
    "        predictions = probs.argmax(1)\n",
    "        \n",
    "        if return_probs:\n",
    "            return predictions, probs\n",
    "        else:\n",
    "            return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.encoder_weights = 'encoder_BENDR.pt'\n",
    "experiment.context_weights = 'contextualizer_BENDR.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receptive field: 143 samples | Downsampled by 96 | Overlap of 47 samples | 16 encoded samples/trial\n"
     ]
    }
   ],
   "source": [
    "model = BENDR(targets=2, samples=1536, channels=20)\n",
    "model.load_all(experiment.encoder_weights, experiment.context_weights, \"classifier_BENDR.pt\")\n",
    "model = model.train(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 331/331 [00:08<00:00, 37.28batch/s]\n"
     ]
    }
   ],
   "source": [
    "preds = model.evaluate(x.to(device)).cpu()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BENDR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "be8fffcb4a1dc2ef8594afdf8f56c6a37005b30d80172b0934bd8081b418c855"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
